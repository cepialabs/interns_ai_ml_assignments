
# ğŸš€ End-to-End Machine Learning: Spam Email Classification and Housing Price Prediction

ğŸ“… **Date:** 12 February 2026

## ğŸ“Œ Project Overview

This project demonstrates the application of **machine learning models** on two different problem types:

1. **Spam Email Classification** (Classification problem)
2. **House Price Prediction** (Regression problem)

The objective is to train **Random Forest models**, compare their performance with baseline models, visualize **feature importance**, and optionally evaluate **XGBoost** models.

---

## ğŸ“‚ Datasets Used

### ğŸ“¨ 1. SpamAssassin Email Dataset

* **Problem Type:** Classification
* **Target Variable:** `label`

  * `1` â†’ Spam
  * `0` â†’ Not Spam (Ham)
* **Features:** Email text content
* **Evaluation Metric:** **F1-score**

**Dataset Source:**
[https://www.kaggle.com/datasets/ganiyuolalekan/spam-assassin-email-classification-dataset](https://www.kaggle.com/datasets/ganiyuolalekan/spam-assassin-email-classification-dataset)

---

### ğŸ  2. Ames Housing Dataset

* **Problem Type:** Regression
* **Target Variable:** `SalePrice`
* **Features:** Numerical and categorical house attributes
* **Evaluation Metric:** **RÂ² Score**

**Dataset Source:**
[https://www.kaggle.com/datasets/prevek18/ames-housing-dataset](https://www.kaggle.com/datasets/prevek18/ames-housing-dataset)

---

## ğŸ§  Models Implemented

### ğŸ”¹ Baseline Models

* **Spam Dataset:** Logistic Regression
* **Housing Dataset:** Linear Regression

### ğŸŒ² Random Forest Models (Primary Models)

* RandomForestClassifier (Spam)
* RandomForestRegressor (Housing)

### ğŸš€ Optional Advanced Model

* XGBoost Classifier
* XGBoost Regressor

---

## âš™ï¸ Machine Learning Pipeline

### Spam Email Classification

1. Text preprocessing using **TF-IDF Vectorization**
2. Train-test split
3. Model training (Logistic Regression â†’ Random Forest â†’ XGBoost)
4. Evaluation using **F1-score** and confusion matrix
5. Visualization of top important words

### House Price Prediction

1. Missing value handling using **SimpleImputer**
2. Categorical encoding using **OneHotEncoder**
3. Pipeline creation using **ColumnTransformer**
4. Model training (Linear Regression â†’ Random Forest â†’ XGBoost)
5. Evaluation using **RÂ² score** and RMSE
6. Visualization of feature importance

---

## ğŸ“Š Results Summary

| Dataset | Model               | Metric   | Performance |
| ------- | ------------------- | -------- | ----------- |
| Spam    | Logistic Regression | F1-score | Baseline    |
| Spam    | Random Forest       | F1-score | Improved    |
| Spam    | XGBoost             | F1-score | Best        |
| Housing | Linear Regression   | RÂ²       | Baseline    |
| Housing | Random Forest       | RÂ²       | Improved    |
| Housing | XGBoost             | RÂ²       | Best        |


```

## ğŸ¯ Key Learnings

* Random Forest models outperform baseline models by capturing non-linear relationships
* Feature importance helps interpret model predictions
* XGBoost provides further performance improvements
* Proper preprocessing is crucial for real-world datasets

---


