{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Day 2 Assignment\n",
        "\n",
        "Datasets:\n",
        "- Ames Housing Dataset (regression)\n",
        "- Spam Assassin Email Classification Dataset (classification)\n",
        "\n",
        "Tasks:\n",
        "- Scale numerical features\n",
        "- Retrain models\n",
        "- Compare performance before vs after scaling"
      ],
      "id": "8d15bd65"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# If needed (run once):\n",
        "# %pip install kagglehub pandas numpy scikit-learn\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ed2f0e28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Download and load Ames Housing dataset (Kaggle) ---\n",
        "import kagglehub\n",
        "\n",
        "AMES_HANDLE = \"prevek18/ames-housing-dataset\"\n",
        "\n",
        "try:\n",
        "    ames_path = kagglehub.dataset_download(AMES_HANDLE)\n",
        "    csv_files = sorted(glob.glob(os.path.join(ames_path, \"**\", \"*.csv\"), recursive=True))\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(\"No CSV files found in Ames dataset.\")\n",
        "\n",
        "    print(\"Ames dataset files:\")\n",
        "    for f in csv_files:\n",
        "        print(\"-\", os.path.basename(f))\n",
        "\n",
        "    ames_csv = csv_files[0]\n",
        "    ames_df = pd.read_csv(ames_csv)\n",
        "    print(\"Selected file:\", os.path.basename(ames_csv))\n",
        "    print(\"Ames dataset loaded:\", ames_df.shape)\n",
        "    display(ames_df.head())\n",
        "except Exception as e:\n",
        "    print(\"Ames dataset download/load failed.\")\n",
        "    print(\"Make sure Kaggle API credentials are configured.\")\n",
        "    print(\"Error:\", e)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c6b14ac7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Regression: compare without vs with scaling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if 'ames_df' in globals():\n",
        "    # Detect target column\n",
        "    target_candidates = [\"SalePrice\", \"saleprice\", \"Price\", \"price\"]\n",
        "    target_col = next((c for c in target_candidates if c in ames_df.columns), None)\n",
        "    if target_col is None:\n",
        "        raise ValueError(\"Could not find target column for price.\")\n",
        "\n",
        "    X = ames_df.select_dtypes(include='number').drop(columns=[target_col], errors='ignore')\n",
        "    y = ames_df[target_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # No scaling\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "    preds = lr.predict(X_test)\n",
        "    print(\"No scaling - MAE:\", mean_absolute_error(y_test, preds))\n",
        "    print(\"No scaling - R2:\", r2_score(y_test, preds))\n",
        "\n",
        "    # With scaling + imputation\n",
        "    scaled_model = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", LinearRegression())\n",
        "    ])\n",
        "    scaled_model.fit(X_train, y_train)\n",
        "    scaled_preds = scaled_model.predict(X_test)\n",
        "    print(\"With scaling - MAE:\", mean_absolute_error(y_test, scaled_preds))\n",
        "    print(\"With scaling - R2:\", r2_score(y_test, scaled_preds))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c4fb5bb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Download and load Spam Assassin dataset (Kaggle) ---\n",
        "SPAM_HANDLE = \"ganiyuolalekan/spam-assassin-email-classification-dataset\"\n",
        "\n",
        "try:\n",
        "    spam_path = kagglehub.dataset_download(SPAM_HANDLE)\n",
        "    spam_files = sorted(glob.glob(os.path.join(spam_path, \"**\", \"*.csv\"), recursive=True))\n",
        "    if not spam_files:\n",
        "        raise FileNotFoundError(\"No CSV files found in spam dataset.\")\n",
        "\n",
        "    print(\"Spam dataset files:\")\n",
        "    for f in spam_files:\n",
        "        print(\"-\", os.path.basename(f))\n",
        "\n",
        "    spam_csv = spam_files[0]\n",
        "    spam_df = pd.read_csv(spam_csv)\n",
        "    print(\"Selected file:\", os.path.basename(spam_csv))\n",
        "    print(\"Spam dataset loaded:\", spam_df.shape)\n",
        "    display(spam_df.head())\n",
        "except Exception as e:\n",
        "    print(\"Spam dataset download/load failed.\")\n",
        "    print(\"Make sure Kaggle API credentials are configured.\")\n",
        "    print(\"Error:\", e)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f2951663"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classification: compare without vs with scaling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if 'spam_df' in globals():\n",
        "    # Detect label column\n",
        "    label_candidates = [\"label\", \"class\", \"spam\", \"target\", \"Category\", \"Prediction\"]\n",
        "    label_col = next((c for c in label_candidates if c in spam_df.columns), None)\n",
        "    if label_col is None:\n",
        "        label_col = spam_df.columns[-1]\n",
        "\n",
        "    X = spam_df.drop(columns=[label_col])\n",
        "    y = spam_df[label_col].astype(str)\n",
        "\n",
        "    # Use numeric features only (word counts) for scaling comparison\n",
        "    X = X.select_dtypes(include='number')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # No scaling\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    print(\"No scaling - Accuracy:\", accuracy_score(y_test, preds))\n",
        "\n",
        "    # With scaling + imputation\n",
        "    scaled_clf = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "    scaled_clf.fit(X_train, y_train)\n",
        "    scaled_preds = scaled_clf.predict(X_test)\n",
        "    print(\"With scaling - Accuracy:\", accuracy_score(y_test, scaled_preds))"
      ],
      "id": "193d613c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}